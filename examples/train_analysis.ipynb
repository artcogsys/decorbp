{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyses properties of decorrelation and whitening methods for decorrelated networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from decorrelation.decorrelation import Decorrelation, DecorLinear, DecorConv2d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from decorrelation.train import decor_train\n",
    "import argparse\n",
    "\n",
    "# automatic reloading of modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(1),\n",
    "        transforms.Normalize((0.5), (0.25)),\n",
    "        # torch.flatten # can be removed when moving to Conv2d\n",
    "        ])\n",
    "\n",
    "train_data = MNIST(root='~/Data', train=True, download=True, transform=transform)\n",
    "train_data = Subset(train_data, np.random.permutation(len(train_data.data))[:1000])\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  \ttime:0.000 s\tbp loss: 0.000000\tdecorrelation loss: 106.560814\n",
      "epoch 1  \ttime:0.554 s\tbp loss: 0.000000\tdecorrelation loss: 92.458618\n",
      "epoch 2  \ttime:0.525 s\tbp loss: 0.000000\tdecorrelation loss: 67.835510\n",
      "epoch 3  \ttime:0.527 s\tbp loss: 0.000000\tdecorrelation loss: 49.946434\n",
      "epoch 4  \ttime:0.549 s\tbp loss: 0.000000\tdecorrelation loss: 37.211742\n",
      "epoch 5  \ttime:0.557 s\tbp loss: 0.000000\tdecorrelation loss: 27.915833\n",
      "epoch 6  \ttime:0.547 s\tbp loss: 0.000000\tdecorrelation loss: 20.952473\n",
      "epoch 7  \ttime:0.524 s\tbp loss: 0.000000\tdecorrelation loss: 15.865977\n",
      "epoch 8  \ttime:0.529 s\tbp loss: 0.000000\tdecorrelation loss: 12.110641\n",
      "epoch 9  \ttime:0.546 s\tbp loss: 0.000000\tdecorrelation loss: 9.355850\n",
      "epoch 10 \ttime:0.538 s\tbp loss: 0.000000\tdecorrelation loss: 7.313002\n"
     ]
    }
   ],
   "source": [
    "model = Decorrelation(784, bias=False, variance=1.0).to(device)\n",
    "\n",
    "lossfun = lambda x, y: nn.Parameter(torch.zeros(1, device=device, dtype=float), requires_grad=True)\n",
    "\n",
    "args = argparse.Namespace(lr=1e-4, eta=1e-5, epochs=10)\n",
    "\n",
    "res = decor_train(args, model, lossfun, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  \ttime:0.000 s\tbp loss: 5.380835\tdecorrelation loss: 106.638420\n",
      "epoch 1  \ttime:0.566 s\tbp loss: 4.126562\tdecorrelation loss: 92.378510\n",
      "epoch 2  \ttime:0.558 s\tbp loss: 2.728143\tdecorrelation loss: 67.550423\n",
      "epoch 3  \ttime:0.567 s\tbp loss: 2.297279\tdecorrelation loss: 50.074562\n",
      "epoch 4  \ttime:0.546 s\tbp loss: 2.049079\tdecorrelation loss: 37.256287\n",
      "epoch 5  \ttime:0.552 s\tbp loss: 1.873144\tdecorrelation loss: 27.816923\n",
      "epoch 6  \ttime:0.580 s\tbp loss: 1.731331\tdecorrelation loss: 20.886711\n",
      "epoch 7  \ttime:0.567 s\tbp loss: 1.619119\tdecorrelation loss: 15.840314\n",
      "epoch 8  \ttime:0.558 s\tbp loss: 1.512893\tdecorrelation loss: 12.159686\n",
      "epoch 9  \ttime:0.579 s\tbp loss: 1.421352\tdecorrelation loss: 9.431981\n",
      "epoch 10 \ttime:0.563 s\tbp loss: 1.346510\tdecorrelation loss: 7.337277\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Sequential):\n",
    "    def __init__(self, in_features, variance):\n",
    "        super().__init__(DecorLinear(in_features, 100, variance=variance))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.view(len(x), -1))\n",
    "    \n",
    "model = Model(784, variance=1.0).to(device)\n",
    "\n",
    "lossfun = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "args = argparse.Namespace(lr=1e-4, eta=1e-5, epochs=10)\n",
    "\n",
    "res = decor_train(args, model, lossfun, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  \ttime:0.000 s\tbp loss: 2.659184\tdecorrelation loss: 111.116158\n",
      "epoch 1  \ttime:0.559 s\tbp loss: 2.598590\tdecorrelation loss: 110.478119\n",
      "epoch 2  \ttime:0.568 s\tbp loss: 2.503471\tdecorrelation loss: 110.325562\n",
      "epoch 3  \ttime:0.561 s\tbp loss: 2.433827\tdecorrelation loss: 110.372955\n",
      "epoch 4  \ttime:0.562 s\tbp loss: 2.385222\tdecorrelation loss: 110.450310\n",
      "epoch 5  \ttime:0.558 s\tbp loss: 2.344632\tdecorrelation loss: 110.519104\n",
      "epoch 6  \ttime:0.558 s\tbp loss: 2.303873\tdecorrelation loss: 110.553780\n",
      "epoch 7  \ttime:0.557 s\tbp loss: 2.271375\tdecorrelation loss: 110.941040\n",
      "epoch 8  \ttime:0.557 s\tbp loss: 2.238182\tdecorrelation loss: 110.781067\n",
      "epoch 9  \ttime:0.555 s\tbp loss: 2.203967\tdecorrelation loss: 111.277313\n",
      "epoch 10 \ttime:0.555 s\tbp loss: 2.173552\tdecorrelation loss: 111.346657\n",
      "epoch 11 \ttime:0.556 s\tbp loss: 2.141980\tdecorrelation loss: 111.523827\n",
      "epoch 12 \ttime:0.557 s\tbp loss: 2.111793\tdecorrelation loss: 112.008949\n",
      "epoch 13 \ttime:0.557 s\tbp loss: 2.081334\tdecorrelation loss: 112.566597\n",
      "epoch 14 \ttime:0.555 s\tbp loss: 2.054578\tdecorrelation loss: 113.064323\n",
      "epoch 15 \ttime:0.557 s\tbp loss: 2.029568\tdecorrelation loss: 113.526939\n",
      "epoch 16 \ttime:0.555 s\tbp loss: 2.001751\tdecorrelation loss: 113.931076\n",
      "epoch 17 \ttime:0.556 s\tbp loss: 1.972577\tdecorrelation loss: 114.308495\n",
      "epoch 18 \ttime:0.556 s\tbp loss: 1.951251\tdecorrelation loss: 115.322098\n",
      "epoch 19 \ttime:0.554 s\tbp loss: 1.919331\tdecorrelation loss: 115.293053\n",
      "epoch 20 \ttime:0.555 s\tbp loss: 1.892023\tdecorrelation loss: 115.829506\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Sequential):\n",
    "    def __init__(self, in_features, variance):\n",
    "        super().__init__(DecorLinear(in_features, 100, bias=True, variance=variance),\n",
    "                        nn.LeakyReLU(),\n",
    "                        DecorLinear(100, 10, bias=True, variance=variance)\n",
    "                        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.view(len(x), -1))\n",
    "    \n",
    "model = Model(784, variance=1.0).to(device)\n",
    "\n",
    "lossfun = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "args = argparse.Namespace(lr=1e-5, eta=1e-5, epochs=20)\n",
    "\n",
    "res = decor_train(args, model, lossfun, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional; and run main experiment on full data; do we have a gain?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis for errors:\n",
    " - MNIST zeros\n",
    " - MNIST scaling (bias?)\n",
    " - variance choice => no normalization would also be an option\n",
    " - optimizer issues\n",
    " - add other learning rule?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
